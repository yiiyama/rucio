#!/usr/bin/env python

from __future__ import division
from __future__ import print_function

import logging
import sys

import argparse

from rucio.core.rse import get_rse_id, get_rse_attribute, update_rse, get_rses_with_attribute
from rucio.core.lock import get_dataset_locks
from rucio.core.replica import list_datasets_per_rse, delete_replicas
from rucio.core.did import list_all_parent_dids
from rucio.core.rule import list_rules, __delete_lock_and_update_replica
from rucio.core.rse_expression_parser import parse_expression

from rucio.common import exception
from rucio.common.config import config_get
from rucio.common.dumper.data_models import Replica as ReplicaDumper
from rucio.common.dumper import smart_open
from rucio.db.sqla.session import transactional_session
from rucio.db.sqla import models

logging.basicConfig(stream=sys.stdout,
                    level=getattr(logging,
                                  config_get('common', 'loglevel',
                                             raise_exception=False,
                                             default='DEBUG').upper()),
                    format='%(asctime)s\t%(process)d\t%(levelname)s\t%(message)s')

def get_parser():
    """
    Returns the argparse parser.
    """
    parser = argparse.ArgumentParser(description='First step for writing a decommissioning agent.')
    parser.add_argument('--rse', action='store', nargs='+', default=[], help='RSE to decommission')
    parser.add_argument('--max_terabytes', action='store', type=float, default=0.01, help='Default throttling.')
    #parser.add_argument('--dry-run', action='store_true', help='Dry run')
    parser.add_argument('--run', action='store_true', help='Actually run')
    return parser

# Policies
JUST_DELETE, MOVE = range(2)

USE_DUMP_FOR_REPLICAS = False

# Keep a tally of deleted volume in this cycle - will exit once we surpass max_terabytes

class MaximumReplicasDeleted(Exception):
    pass

class MaximumRulesDeleted(Exception):
    pass

class MaximumDatasetsDeleted(Exception):
    pass

class LimitChecker(object):
    def __init__(self, replicas=-1, rules=-1, datasets=-1):
        self.max_replicas = replicas
        self.max_rules = rules
        self.max_datasets = datasets

        self.deleted_replicas = 0
        self.deleted_rules = 0
        self.deleted_datasets = 0
        
    def replica(self):
        self.deleted_replicas += 1
        if self.deleted_replicas == self.max_replicas:
            raise MaximumReplicasDeleted(self.max_replicas, self.deleted_replicas)
        
    def rule(self):
        self.deleted_rules == 1
        if self.deleted_rules > self.max_rules:
            raise MaximumRulesDeleted(self.max_rules, self.deleted_rules)

    def dataset(self):
        self.deleted_datasets == 1
        if self.deleted_datasets > self.max_datasets:
            raise MaximumDatasetsDeleted(self.max_datasets, self.deleted_datasets)

    def any(self):
        return self.deleted_replicas != 0 or self.deleted_rules != 0 or self.deleted_datasets != 0
    

@transactional_session
def prepare_rse_for_decommissioning(rse_id, log_header='', dry_run=False, session=None):
    """
    Get the RSE ready for decommissioning (availability and greedyDeletion)
    """
    
    # Set the RSE attributes
    for attr, value in [('availability_read', True), ('availability_write', False), ('availability_delete', True), ('greedyDeletion', True)]:
        current = get_rse_attribute(attr, rse_id=rse_id, session=session)
        if current == value:
            continue
            
        logging.debug('%(log_header)supdating attribute %(attr)s (%(current)s => %(value)s)' % locals())
        if not dry_run:
            update_rse(rse_id, {attr: value}, session=session)
            
            
@transactional_session
def mark_rse_as_done(rse_id, log_header='', dry_run=False, session=None):
    logging.debug('%(log_header)supdating attribute decommission to "complete"' % locals())
    if not dry_run:
        try:
            del_rse_attribute(rse_id, 'decommission', session=session)
        except:
            pass
        else:
            add_rse_attribute(rse_id, 'decommission', 'complete', session=session)
        
        
@transactional_session
def delete_single_rse_rules(rse, limit_check, move_dest=None, log_header='', dry_run=False, session=None):
    """
    Delete rules with RSE expressions that consist only of the name of the given RSE.
    
    :param rse:          Name of the RSE.
    :param limit_check:  LimitChecker instance
    :param session:      The database session.
    """

    for rule in list_rules(filters={'rse_expression': rse}, session=session):
        rule_id = rule['id']
        scope = rule['scope']
        name = rule['name']

        if move_dest is None:
            logging.debug('%(log_header)sDeleting rule %(rule_id)s for dataset %(scope)s:%(name)s' % locals())
            if not dry_run:
                delete_rule(rule_id, delete_parent=True, session=session)
        else:
            pass

        limit_check.rule()
        

@transactional_session
def process_multi_rse_rules(rse, limit_check, log_header='', dry_run=False, session=None):
    """
    Find rules with RSE expressions involving the given RSE among others.
    """
    
    for rule in list_rules(session=session):
        for rse_info in parse_expression(rule['rse_expression'], session=session):
            if rse_info['rse'] == rse:
                # The rse_expression for this rule contains the current RSE among others
                logging.warning('%(log_header)sNontrivial rule for dataset %(scope)s:%(name)s. Deferring action' % locals())
                break

                
# @transactional_session
# def delete_stale_locks(rse_id, limit_check, log_header='', dry_run=False, session=None):
#     """
#     Loop over the datasets with no rules and see if there are out-of-sync locks.
#     This function should only be called once all the rules on the RSE are deleted
#     (otherwise list_datasets_per_rse is too expensive).
#     """
    
#     for dataset in list_datasets_per_rse(rse_id, session=session):
#         scope = dataset['scope']
#         name = dataset['name']
        
#         # Make sure this dataset does not have a rule at the RSE
#         for rule in list_rules(filters={'scope': scope, 'name': name}, session=session):
#             for rse_info in parse_expression(rule['rse_expression'], session=session):
#                 if rse_info['id'] == rse_id:
#                     raise exception.RucioException('delete_stale_locks should be called after all rules on the RSE are deleted.')
                    
#         # What should I do here?

#         # Loop over locks for this dataset
#         query = session.query(models.DatasetLock.rse_id,
#                               models.DatasetLock.scope,
#                               models.DatasetLock.name,
#                               models.DatasetLock.rule_id,
#                               models.DatasetLock.account,
#                               models.DatasetLock.state,
#                               models.DatasetLock.length,
#                               models.DatasetLock.bytes,
#                               models.DatasetLock.accessed_at)\
#             .filter_by(scope=scope, name=name, rse_id=rse_id)

#             logging.debug('RSE: %(rse)s -- Deleting out-of-sync lock for dataset %(scope)s:%(name)s' % locals())
#             if not dry_run:
#                 lock_obj = session.query(models.ReplicaLock)\
#                                   .filter_by(scope=scope, name=name, rse_id=rse_id).one()[0]
#                 __delete_lock_and_update_replica(lock=lock_obj)

#             check_volume(dataset['bytes'])
    
#     for dataset in datasets_with_no_rule:

#             break
            

@transactional_session
def decommission_rse(rse, profile, max_terabytes=None, dry_run=False, session=None):
    """
    Set RSE attributes for decommissioning (read=True, write=False, delete=True).

    :param rse:      Name of the RSE.
    :param session:  The database session.
    """

    logging.info('Decommissioning: Preparing RSE %(rse)s' % locals())
    
    log_header = 'RSE: %(rse)s -- ' % locals()
    
    try:
        rse_id = get_rse_id(rse, session=session, include_deleted=False)
    except exception.RSENotFound:
        logging.warning('RSE %(rse)s not found' % locals())
        return False

    # Set the RSE attributes to stop accepting writes and delete greedily
    prepare_rse_for_decommissioning(rse_id, log_header=log_header, dry_run=dry_run, session=session)

    limit_check = LimitChecker()

    try:
        if profile == JUST_DELETE:
            # 1. Delete rules where the RSE expression is the RSE itself
            delete_single_rse_rules(rse, limit_check, log_header=log_header, dry_run=dry_run, session=session)
            
            # 2. Deal with rules with more complicated RSE expressions
            process_multi_rse_rules(rse, limit_check, log_header=log_header, dry_run=dry_run, session=session)
            
            if limit_check.deleted_rules == 0:
                # 3. There were no rules to be deleted in this cycle - move on to final cleanup
                # delete_locks_with_no_rule()
                # delete_replicas_with_no_locks()
            
        elif profile == MOVE:
            pass
    
    except MaximumReplicasDeleted as exc:
        limit = exc.args[0]
        deleted = exc.args[1]
        logging.info('RSE: %(rse)s -- Stopping decommissioning cycle because the number of deleted replicas %(deleted)s reached the limit %(limit)s' % locals())
        return False
    
    except MaximumRulesDeleted as exc:
        limit = exc.args[0]
        deleted = exc.args[1]
        logging.info('RSE: %(rse)s -- Stopping decommissioning cycle because the number of deleted rules %(deleted)s reached the limit %(limit)s' % locals())
        return False
    
    except MaximumDatasetsDeleted as exc:
        limit = exc.args[0]
        deleted = exc.args[1]
        logging.info('RSE: %(rse)s -- Stopping decommissioning cycle because the number of deleted datasets %(deleted)s reached the limit %(limit)s' % locals())
        return False

    if not limit_check.any():
        # Mark the RSE as decommissioning complete
        mark_rse_as_done(rse_id, log_header=log_header, dry_run=dry_run, session=session)

    return True


def decommission_rses(rses=[], max_terabytes=None, dry_run=True):
    rses_to_decommission = {}
    
    profile_dict = {'just_delete': JUST_DELETE, 'move': MOVE}
    
    # Loop over the RSEs with decommission attribute set
    for rse in get_rses_with_attribute('decommission'):
        if rse['decommission'] == 'complete':
            continue
            
        profile = profile_dict[rse['decommission']]
            
        try:
            rse_max_terabytes = rse['decommission_throttle']
        except KeyError:
            rse_max_terabytes = max_terabytes

        rses_to_decommission[rse['rse']] = (profile, rse_max_terabytes)

    # Override by the function argument
    for entry in rses:
        if type(entry) is tuple:
            try:
                rse, profile, rse_max_terabytes = entry
            except ValueError:
                rse, profile = entry
                rse_max_terabytes = max_terabytes
        else:
            rse = entry
            profile = JUST_DELETE
            rse_max_terabytes = max_terabytes
            
        rses_to_decommission[rse] = (profile, rse_max_terabytes)
        
    logging.info('RSEs to decommission: %(rses_to_decommission)s' % locals())
     
    # Act on the RSEs
    for rse, (profile, rse_max_terabytes) in rses_to_decommission.items():
        decommission_rse(rse, profile, max_terabytes=rse_max_terabytes, dry_run=dry_run)

        
if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    
    decommission_rses(rses=args.rse, max_terabytes=args.max_terabytes, dry_run=(not args.run))