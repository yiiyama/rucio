#!/usr/bin/env python

from __future__ import division
from __future__ import print_function

import os
import sys
import re
from datetime import datetime
import logging
import argparse
import pickle

from sqlalchemy import or_, and_
from sqlalchemy.sql import func
from sqlalchemy.orm.exc import NoResultFound

from rucio.core.rse import get_rse_id, get_rse_attribute, update_rse, add_rse_attribute, del_rse_attribute, get_rses_with_attribute
from rucio.core.rule import update_rule
from rucio.core.rse_expression_parser import parse_expression
from rucio.core.lock import get_dataset_locks_by_rse_id
from rucio.common import exception
from rucio.common.config import config_get
from rucio.db.sqla import models
from rucio.db.sqla.session import read_session, stream_session, transactional_session
from rucio.db.sqla.constants import DIDType

logging.basicConfig(stream=sys.stdout,
                    level=getattr(logging,
                                  config_get('common', 'loglevel',
                                             raise_exception=False,
                                             default='INFO').upper()),
                    format='%(asctime)s\t%(process)d\t%(levelname)s\t%(message)s')

# TBD - we should cache the RSEFileAssociation scan results and redo the scan only when all cached replicas are gone
CACHE_DIR = '/tmp'

# Decommissioning profiles
JUST_DELETE, MOVE = range(2)
PROFILES = ['just_delete', 'move']

def get_parser():
    """
    Returns the argparse parser.
    """
    parser = argparse.ArgumentParser(description='First step for writing a decommissioning agent.')
    parser.add_argument('--rse', dest='rses', metavar='RSE', action='store', nargs='+', default=[], help='RSE to decommission')
    parser.add_argument('--max-replicas', dest='max_replicas', action='store', type=int, default=-1, help='Maximum number of replicas to delete per cycle.')
    parser.add_argument('--max-rules', dest='max_rules', action='store', type=int, default=-1, help='Maximum number of rules to delete per cycle.')
    parser.add_argument('--max-datasets', dest='max_datasets', action='store', type=int, default=-1, help='Maximum number of datasets to delete per cycle.')
    parser.add_argument('--force', dest='force', action='store_true', help='Force decommissioning under the given profile.')
    #parser.add_argument('--dry-run', action='store_true', help='Dry run')
    parser.add_argument('--run', action='store_true', help='Actually run')
    parser.add_argument('--log-level', dest='log_level', default='debug', help='Log level as a string.')
    return parser

    
class DeletionLimitExceeded(Exception):
    def __init__(self, what, limit, count):
        self.what = what
        self.limit = limit
        self.count = count
        
    def __str__(self):
        return 'number of deleted %s=%s reached the limit=%s' % (self.what, self.count, self.limit)


class LimitChecker(object):
    def __init__(self, rules, datasets, replicas):
        self.max_rules = rules
        self.max_datasets = datasets
        self.max_replicas = replicas

        self.deleted_rules = 0
        self.deleted_datasets = 0
        self.deleted_replicas = 0
        
        self.local_deleted_rules = 0
        self.local_deleted_datasets = 0
        self.local_deleted_replicas = 0
        
    def increment(self, rules=0, datasets=0, replicas=0):
        self.deleted_rules += (rules if rules else 0)
        self.deleted_datasets += (datasets if datasets else 0)
        self.deleted_replicas += (replicas if replicas else 0)
        
        self.local_deleted_rules += (rules if rules else 0)
        self.local_deleted_datasets += (datasets if datasets else 0)
        self.local_deleted_replicas += (replicas if replicas else 0)
        
        if self.max_rules >= 0 and self.deleted_rules >= self.max_rules:
            raise DeletionLimitExceeded('rules', self.max_rules, self.deleted_rules)

        if self.max_datasets >= 0 and self.deleted_datasets >= self.max_datasets:
            raise DeletionLimitExceeded('datasets', self.max_datasets, self.deleted_datasets)

        if self.max_replicas >= 0 and self.deleted_replicas >= self.max_replicas:
            raise DeletionLimitExceeded('replicas', self.max_replicas, self.deleted_replicas)

    def any(self):
        return self.deleted_rules + self.deleted_datasets + self.deleted_replicas != 0
    
    def start_local_count(self):
        """Local count within some code block"""
        self.local_deleted_rules = 0
        self.local_deleted_datasets = 0
        self.local_deleted_replicas = 0
        
    def local_any(self):
        return self.local_deleted_rules + self.local_deleted_datasets + self.local_deleted_replicas != 0
    

@transactional_session
def set_rse_availability(rse, rse_id, dry_run=False, session=None):
    """
    Get the RSE ready for decommissioning (availability and greedyDeletion)
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    # Check the RSE availability attributes (in the RSE table)
    result = session.query(models.RSE).filter_by(id=rse_id).one()
    # hard-coding potentially changeable mapping..! Check rucio.core.rse
    read, write, delete = 4, 2, 1
    availability = (read | delete)
    if result['availability'] != availability:
        logger.debug('Updating RSE availability to read !write delete')
        parameters = {'availability_read': True, 'availability_write': False, 'availability_delete': True}
        if not dry_run:
            update_rse(rse_id, parameters, session=session)

    # Check the RSE greedyDeletion attribute (in the RSEAttrAssociation table)
    try:
        current = get_rse_attribute('greedyDeletion', rse_id=rse_id, session=session)[0]
    except IndexError:
        current = False
        
    if current != True:
        logger.debug('Updating attribute greedyDeletion to True')
        if not dry_run:
            add_rse_attribute(rse_id, 'greedyDeletion', True, session=session)

        
@transactional_session
def mark_rse_as_done(rse, rse_id, dry_run=False, session=None):
    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    logger.debug('Updating attribute decommission to "complete"')
    if not dry_run:
        # update_rse can only update attributes in the RSE table
        # custom RSE attributes are stored in RSEAttrAssociation
        # add_rse_attribute uses session.merge() which will update the attribute value if the key already exists
        add_rse_attribute(rse_id, 'decommission', 'complete', session=session)


# Perhaps the following function can be moved to core.rule with some generalization? (Y.I. 02.04.2021)
@stream_session
def get_dataset_rules_by_rse_id(rse_id, selection=None, session=None):
    query = session.query(models.ReplicationRule).\
        with_hint(models.DatasetLock, "INDEX(DATASET_LOCKS DATASET_LOCKS_RSE_ID_IDX)", 'oracle').\
        join(models.DatasetLock, models.ReplicationRule.id == models.DatasetLock.rule_id).\
        filter(models.DatasetLock.rse_id == rse_id)
    
    if type(selection) is tuple:
        query = query.filter(*selection)
    elif selection is not None:
        query = query.filter(selection)
    
    for rule in query.yield_per(5):
        yield rule.to_dict()


@stream_session
def get_file_rules_by_rse_id(rse_id, selection=None, cache_path=None, session=None):
    """
    Perform a full scan of the RSEFileAssociation table to collect all replicas at the RSE. To be executed sparingly,
    only when we have no more rules to delete.
    
    :param rse_id: RSE ID.
    :param cache_path: None or the path of the cache pickle file.
    """

    query = session.query(models.ReplicationRule).\
        join(models.ReplicaLock, models.ReplicationRule.id == models.ReplicaLock.rule_id).\
        filter(models.ReplicaLock.rse_id == rse_id)

    if type(selection) is tuple:
        query = query.filter(*selection)
    elif selection is not None:
        query = query.filter(selection)

    if cache_path is not None:
        rules = []
        for rule in query.yield_per(100):
            rules.append(rule.to_dict())

        with open(cache_path, 'wb') as out:
            pickle.dump(rules, out, protocol=pickle.HIGHEST_PROTOCOL)

        for rule in rules:
            yield rule

    else:
        for rule in query.yield_per(5):
            yield rule.to_dict()


@stream_session
def get_file_rules_from_replicas(replicas, batch_size=1000, session=None):
    """
    Look up the rules locking the replicas.

    :param replicas: List of replicas (dict).
    :param batch_size: Optional batch size for lookup query.
    """

    conditions = []

    for replica in replicas:
        conditions.append(and_(
            models.ReplicaLock.scope == replica['scope'],
            models.ReplicaLock.name == replica['name'],
            models.ReplicaLock.rse_id == replica['rse_id']
        ))

    if batch_size <= 0:
        batch_size = len(conditions)
        
    begin = 0
    end = batch_size

    while begin < len(conditions):
        query = session.query(models.ReplicationRule).\
            with_hint(models.ReplicaLock, "INDEX(LOCKS LOCKS_PK)", 'oracle').\
            join(models.ReplicaLock, models.ReplicationRule.id == models.ReplicaLock.rule_id).\
            filter(or_(*conditions[begin:end]))

        counts = 0
        for row in query.yield_per(100):
            counts += 1
            yield row.to_dict()

        print(counts, 'rows from', len(conditions[begin:end]))

        begin += end
        end += batch_size


@read_session
def __container_to_datasets(scope, name, max_depth=0, _depth=0, session=None):
    """Flatten a container to a list of datasets.

    ``scope`` should be a ``str`` with the scope of the container.

    ``name`` should be a ``str`` with the name of the container.

    ``max_depth`` should be a positive ``int`` to limit the recursion depth (default: no limit).

    ``session`` should be an ``sqlalchemy.orm.session.Session``-like
    object.  If not provided, one will be created automatically.

    Returns a ``list`` of ``tuples``s (scope, name).
    """
    
    if max_depth > 0 and _depth > max_depth:
        raise RuntimeError('exceeded maximum depth')
        
    query = session.query(models.DataIdentifierAssociation.child_scope,
                          models.DataIdentifierAssociation.child_name,
                          models.DataIdentifierAssociation.child_type).\
        with_hint(models.DataIdentifierAssociation, "INDEX(CONTENTS CONTENTS_PK)", 'oracle').\
        filter(models.DataIdentifierAssociation.scope == scope,
               models.DataIdentifierAssociation.name == name,
               or_(models.DataIdentifierAssociation.child_type == DIDType.CONTAINER,
                   models.DataIdentifierAssociation.child_type == DIDType.DATASET))
    
    datasets = set()

    for child_scope, child_name, child_type in query.all():
        if child_type == DIDType.CONTAINER:
            datasets.update(__container_to_datasets(child_scope, child_name, max_depth, _depth + 1, session))
        elif child_type == DIDType.DATASET:
            datasets.add((child_scope, child_name))
            
    return datasets


@read_session
def __count_rule(limit_check, rule, logger, session=None):
    """
    Increment the counters for throttling from a rule
    """
    
    if rule['did_type'] == DIDType.FILE:
        logger.debug('DID is a file')
        
        limit_check.increment(rules=1, replicas=1)

    elif rule['did_type'] in [DIDType.DATASET, DIDType.CONTAINER]:
        # This DID is a dataset or a container
        # We count all the constituents regardless of the replication state to throttle on the maximum load
        if rule['did_type'] == DIDType.DATASET:
            dataset_condition = [and_(models.DataIdentifier.scope == rule['scope'], models.DataIdentifier.name == rule['name'])]
        elif rule['did_type'] == DIDType.CONTAINER:
            dataset_condition = []
            for s, n in __container_to_datasets(rule['scope'], rule['name'], session=session):
                dataset_condition.append(
                    and_(models.DataIdentifier.scope == s, models.DataIdentifier.name == n)
                )

        num_datasets = len(dataset_condition)

        num_files = 0
        if num_datasets != 0:
            num_files, = session.query(func.sum(models.DataIdentifier.length)).\
                filter(or_(*dataset_condition)).one()

        logger.debug('DID contains %(num_datasets)s datasets and %(num_files)s files' % locals())
        
        # Finally increment the counters
        limit_check.increment(rules=1, datasets=num_datasets, replicas=num_files)
        
    else:
        # Unhandled DID type - warn and just increment the rules counter
        type_str = str(rule['did_type']).lower()
        logger.warning('DID type %(type_str)s found, but we can handle only FILE, DATASET, and CONTAINER types at the moment.' % locals())
        limit_check.increment(rules=1)


@read_session
def __count_replica_rses(scope, name, did_type, batch_size=1000, session=None):
    # Create filter conditions for replicas

    if did_type == DIDType.FILE:
        file_condition = [and_(models.RSEFileAssociation.scope == scope,
                               models.RSEFileAssociation.name == name)]

    elif did_type in [DIDType.DATASET, DIDType.CONTAINER]:
        if did_type == DIDType.DATASET:
            dataset_condition = [and_(models.DataIdentifierAssociation.scope == scope, models.DataIdentifierAssociation.name == name)]
        elif did_type == DIDType.CONTAINER:
            dataset_condition = []
            for s, n in __container_to_datasets(scope, name, session=session):
                dataset_condition.append(
                    and_(models.DataIdentifierAssociation.scope == s, models.DataIdentifierAssociation.name == n)
                )

        if len(dataset_condition) == 0:
            logger.warning('Empty container %(scope)s:%(name)s' % locals())
            return 0

        query = session.query(models.DataIdentifierAssociation.child_scope,
                              models.DataIdentifierAssociation.child_name).\
                with_hint(models.DataIdentifierAssociation, "INDEX(CONTENTS CONTENTS_PK)", 'oracle').\
                filter(or_(*dataset_condition))

        file_condition = []
        for child_scope, child_name in query.yield_per(10):
            file_condition.append(and_(models.RSEFileAssociation.scope == child_scope,
                                       models.RSEFileAssociation.name == child_name))

    else:
        # Unhandled DID type - warn and return 0
        type_str = str(did_type).lower()
        logger.warning('DID %(scope)s:%(name)s is type %(type_str)s, but we can handle only FILE, DATASET, and CONTAINER types at the moment.' % locals())
        return 0

    # Collect all rse ids

    rse_ids = set()

    if batch_size <= 0:
        batch_size = len(file_condition)

    start = 0
    end = batch_size

    while start < len(file_condition):
        query = session.query(models.RSEFileAssociation.rse_id).\
            filter(or_(*file_condition[start:end]))

        for rse_id in query.yield_per(100):
            rse_ids.add(rse_id)

        start = end
        end += batch_size

    return len(rse_ids)
        

@read_session
def count_rules_pending_deletion(rse, rules, limit_check, session=None):
    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    remaining_rules = []

    # Start counting how many rules, datasets, and replicas we delete in this function
    limit_check.start_local_count()
    
    for rule in rules:
        if rule['expires_at'] is None or rule['expires_at'] >= datetime.utcnow():
            remaining_rules.append(rule)
            continue
        
        rule_id = rule['id']
        scope = rule['scope']
        name = rule['name']
        type_str = str(rule['did_type']).lower()

        logger.debug('Rule %(rule_id)s for %(type_str)s %(scope)s:%(name)s is bound for deletion' % locals())
        
        __count_rule(limit_check, rule, logger, session=session)

    if limit_check.local_any():
        num_rules = limit_check.local_deleted_rules
        logger.info('%(num_rules)s rules are bound for deletion' % locals())
    else:
        logger.info('No pending deletions')
        
    return remaining_rules

        
@transactional_session
def delete_single_rse_rules(rse, rules, limit_check, dry_run=False, session=None):
    """
    Delete rules with RSE expressions that consist only of the name of the given RSE.
    
    :param rse:          Name of the RSE.
    :param rules:        List of rules as dictionaries.
    :param limit_check:  LimitChecker instance
    :param session:      The database session.
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    remaining_rules = []

    # Start counting how many rules, datasets, and replicas we delete in this function
    limit_check.start_local_count()
    
    for rule in rules:
        if rule['rse_expression'] != rse:
            remaining_rules.append(rule)
            continue
            
        rule_id = rule['id']
        scope = rule['scope']
        name = rule['name']
        type_str = str(rule['did_type']).lower()

        logger.debug('Deleting rule %(rule_id)s for %(type_str)s %(scope)s:%(name)s' % locals())
        if not dry_run:
            options = {'lifetime': 0, 'state': 'suspended', 'cancel_requests': True}
            update_rule(rule_id, options, session=session)
            
        __count_rule(limit_check, rule, logger, session=session)

    if limit_check.local_any():
        num_rules = limit_check.local_deleted_rules
        logger.info('Deleted %(num_rules)s rules' % locals())
    else:
        logger.info('Found no trivial rule to delete')
        
    return remaining_rules


@transactional_session
def delete_rules_with_one_replica(rse, rules, limit_check, dry_run=False, session=None):
    """
    Delete rules which ended up with only one replica globally, sitting at the given RSE.
    
    :param rse:          Name of the RSE.
    :param rules:        List of rules as dictionaries.
    :param limit_check:  LimitChecker instance
    :param session:      The database session.
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())

    remaining_rules = []

    limit_check.start_local_count()

    for rule in rules:
        scope = rule['scope']
        name = rule['name']
        did_type = rule['did_type']
        
        # For each rule, expand the list of (file) replicas and identify their rse_ids
        num_rses = __count_replica_rses(scope, name, did_type, session=session)

        if num_rses != 1:
            # If there are more than one (or zero) rse, skip this rule
            remaining_rules.append(rule)
            continue

        # The single RSE is assumed to be the one we are decommissioning now

        rule_id = rule['id']
        type_str = str(did_type).lower()

        logger.debug('Deleting rule %(rule_id)s for %(type_str)s %(scope)s:%(name)s' % locals())
        if not dry_run:
            options = {'lifetime': 0, 'state': 'suspended', 'cancel_requests': True}
            update_rule(rule_id, options, session=session)
            
        __count_rule(limit_check, rule, logger, session=session)

    if limit_check.local_any():
        num_rules = limit_check.local_deleted_rules
        logger.info('Deleted %(num_rules)s rules' % locals())
    else:
        logger.info('Found no rule to delete')
        
    return remaining_rules


@transactional_session
def process_multi_rse_rules(rse, rse_id, limit_check, dry_run=False, session=None):
    """
    Find rules with RSE expressions involving the given RSE among others.
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    limit_check.start_local_count()
    
    all_rse_names = list(r for (r,) in session.query(models.RSE.rse).all())
    
    query = session.query(models.ReplicationRule).\
        filter(models.ReplicationRule.rse_expression.notin_(session.query(models.RSE.rse)))
    
    lines = 0
    
    for rule in query:
        rule = rule.to_dict()
        rse_expression = rule['rse_expression']
        logger.debug('RSE expression: "%(rse_expression)s' % locals())
        lines += 1
        if lines == 200:
            break
            
        continue
        if rse_expression in all_rse_names:
            # Skip all trivial (single RSE name) rules
            continue
            
        try:
            rse_info_list = parse_expression(rse_expression, session=session)
        except exception.InvalidRSEExpression:
            continue
            
        for rse_info in rse_info_list:
            if rse_info['rse'] == rse:
                scope = rule['scope']
                name = rule['name']
                # The rse_expression for this rule contains the current RSE among others
                logger.warning('Nontrivial rule for dataset %(scope)s:%(name)s. Deferring action' % locals())
                logger.debug('RSE expression is "%(rse_expression)s"' % locals())
                limit_check.increment(rules=1) # Otherwise we'll mark this RSE as decommission=complete
                break
                
    if not limit_check.local_any(reset=True):
        logger.info('Found no nontrivial rule to delete')
        

@read_session
def scan_for_replicas(rse, rse_id, cache_path=None, dry_run=False, session=None):
    """
    Perform a full scan of the RSEFileAssociation table to collect all replicas at the RSE. To be executed sparingly,
    only when we have no more rules to delete.
    
    :param rse_id: RSE ID.
    :param cache_path: None or the path of the cache pickle file.
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())

    replicas = []
    
    # Even though this is a read operation, it's a costly one
    # -> Let's not actually run the query in dry run mode.
    if not dry_run:
        logger.info('Scanning the RSEFileAssociation table for remaining replicas on this RSE. This will take a few minutes..')
        
        result = session.query(models.RSEFileAssociation).\
            filter(models.RSEFileAssociation.rse_id == rse_id).yield_per(100)

        for row in result:
            replicas.append(row.to_dict())
        
    if cache_path is not None and not dry_run:
        with open(cache_path, 'wb') as out:
            pickle.dump(replicas, out, protocol=pickle.HIGHEST_PROTOCOL)

    num_remaining_replicas = len(replicas)
    logger.info('%(num_remaining_replicas)s replicas remain on the RSE' % locals())

    return replicas


def __check_cached_objects(cache_path, query, row_condition, batch_size=1000, dry_run=False):
    with open(cache_path, 'rb') as source:
        cached_objects = pickle.load(source)
        
    if len(cached_objects) == 0:
        return [], 0

    conditions = []
    for row in cached_objects:
        conditions.append(and_(*row_condition(row)))
       
    remaining = []

    if batch_size <= 0:
        batch_size = len(conditions)
        
    begin = 0
    end = batch_size

    while begin < len(conditions):
        result = query.filter(or_(*conditions[begin:end])).yield_per(100)

        begin = end
        end += batch_size

        for row in result:
            remaining.append(row.to_dict())

    if not dry_run:
        if len(remaining) == 0:
            # Delete the cache file
            os.unlink(cache_path)
        else:
            # Update the cache file with the new list
            with open(cache_path, 'wb') as out:
                pickle.dump(remaining, out, protocol=pickle.HIGHEST_PROTOCOL)

    return remaining, len(conditions)


@read_session
def check_cached_replica_rules(rse, rse_id, cache_path, batch_size=1000, dry_run=False, session=None):
    """
    Read the cached list of locks-replicas and check their existence in the ReplicaLock and ReplicationRule tables.
    

    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())

    query = session.query(models.ReplicationRule).\
        join(models.ReplicaLock, models.ReplicationRule.id == models.ReplicaLock.rule_id)

    row_condition = lambda row: \
        (models.ReplicationRule.scope == row['scope'],
         models.ReplicationRule.name == row['name'],
         models.ReplicationRule.rule_id == row['rule_id'],
         models.ReplicaLock.rse_id == rse_id)

    remaining_rules, num_cached = __check_cached_objects(cache_path, query, row_condition, dry_run=dry_run)

    if num_cached == 0:
        logger.info('There are no cached rules. RSE has likely been emptied in the previous run.')
        return []

    if len(remaining_rules) == 0:
        logger.info('All %(num_cached)s cached rules are gone' % locals())
        return []

    num_remaining = len(remaining_rules)
    logger.info('%(num_remaining)s rules remain on the RSE' % locals())

    return remaining_rules
    
    
@read_session
def check_cached_replicas(rse, rse_id, cache_path, batch_size=1000, dry_run=False, session=None):
    """
    Read the cached list of replicas and check their existence in the RSEFileAssociation table.
    
    :param rse_id: RSE ID.
    :param cache_path: Path of the cache pickle file.
    :param batch_size: Query in batch of size N. Empirically repeating batches of ~1000 seems more efficient than
        querying for a larger full list. Set to <= 0 to disable batching.
    """

    logger = logging.getLogger('decommissioning.%(rse)s' % locals())

    query = session.query(models.RSEFileAssociation)

    row_condition = lambda row: \
        (models.RSEFileAssociation.scope == row['scope'],
         models.RSEFileAssociation.name == row['name'],
         models.RSEFileAssociation.rse_id == rse_id)

    remaining_replicas, num_cached = __check_cached_objects(cache_path, query, row_condition, dry_run=dry_run)

    if num_cached == 0:
        logger.info('There are no cached replicas. RSE has likely been emptied in the previous run.')
        return []

    if len(remaining_replicas) == 0:
        logger.info('All %(num_cached)s cached replicas are gone' % locals())
        return []

    num_remaining = len(remaining_replicas)
    logger.info('%(num_remaining)s replicas remain on the RSE' % locals())

    return remaining_replicas


@transactional_session
def delete_rules(rse, rules, limit_check, dry_run=False, session=None):
    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    try:
        # 2.1 Count rules pending deletion
        logger.info('Counting rules pending deletion')
        rules = count_rules_pending_deletion(rse, rules, limit_check, session=session)
        
        # 2.2 Delete rules where the RSE expression is the RSE itself
        logger.info('Deleting trivial rules on the RSE')
        rules = delete_single_rse_rules(rse, rules, limit_check, dry_run=dry_run, session=session)
        
        # 2.3 Delete rules where the replica exists only at this RSE, even though the expression is more general
        logger.info('Deleting rules resulting in single replicas')
        rules = delete_rules_with_one_replica(rse, rules, limit_check, dry_run=dry_run, session=session)

    except DeletionLimitExceeded as exc:
        reason = str(exc)
        logger.info('Stopping decommissioning cycle because %(reason)s' % locals())
        return False

    if limit_check.any():
        num_rules = limit_check.deleted_rules
        logger.info('%(num_rules)s rules are being deleted. Decommissioning is not complete.' % locals())
        return False
    
    elif len(rules) != 0:
        num_remaining_rules = len(rules)
        # Nothing was deleted in this round but there are rules that we did not know how to delete
        logger.info('%(num_remaining_rules)s rules are being held because we do not know how to delete them.' % locals())
        return False

    return True


@transactional_session    
def decommission_rse_just_delete(rse, rse_id, limit_check, dry_run=False, session=None):
    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    
    # 1. Set RSE availability and greedyDeletion attributes
    logger.info('Setting RSE availability flags')
    set_rse_availability(rse, rse_id, dry_run=dry_run, session=session)

    # 2. Identify replication rules through dataset replicas and delete 
    logger.info('Collecting rules on datasets on the RSE')
    rules = get_dataset_rules_by_rse_id(rse_id, session=session)

    complete = delete_rules(rse, rules, limit_check, dry_run=dry_run, session=session)

    if not complete:
        return False

    USE_REPLICA_LOCKS = False

    if USE_REPLICA_LOCKS:
        # 3. Identify replication rules through (file) replicas and delete
        logging.info('Collecting rules on individual files on the RSE')

        cache_path = CACHE_DIR + ('/replicarules_%(rse)s.pkl' % locals())
        if os.path.exists(cache_path):
            # There is a cached list of file-level rules from a previous scan. Let's check those rules first
            rules = check_cached_replica_rules(rse, rse_id, cache_path, batch_size=1000, dry_run=dry_run, session=session)
    
        if len(rules) == 0:
            logger.info('Scanning the ReplicaLock table for remaining replicas on this RSE. This will take a few minutes..')
            # Even though this is a read operation, it's a costly one
            # -> Let's not actually run the query in dry run mode.
            if not dry_run:
                rules = get_file_rules_by_rse_id(rse_id, cache_path=cache_path, session=session)

    else:
        # ReplicaLock table is not indexed by RSE, just like RSEFileAssociation (replicas). We may want to limit expensive
        # full-table scans to minimum -> Do the replicas scan here and look up the rules locking those replicas. The list
        # of replicas can then be cached and used later

        # 3. Identify replication rules through (file) replicas and delete
        logging.info('Collecting rules on individual files on the RSE')

        cache_path = CACHE_DIR + ('/replicas_%(rse)s.pkl' % locals())
        if os.path.exists(cache_path):
            # There is a cached list of file-level rules from a previous scan. Let's check those rules first
            replicas = check_cached_replicas(rse, rse_id, cache_path, batch_size=1000, dry_run=dry_run, session=session)
        else:
            replicas = scan_for_replicas(rse, rse_id, cache_path=cache_path, dry_run=dry_run, session=session)

        rules = get_file_rules_from_replicas(replicas, batch_size=1000, session=session)

    complete = delete_rules(rse, rules, limit_check, dry_run=dry_run, session=session)

    if not complete:
        return False
    
    # There are no more rules - now check if there are remaining replicas on the RSE
    replicas = []

    cache_path = CACHE_DIR + ('/replicas_%(rse)s.pkl' % locals())
    if os.path.exists(cache_path):
        # There is a cached list of replicas from a previous scan. Let's check those replicas first
        replicas = check_cached_replicas(rse, rse_id, cache_path, batch_size=1000, dry_run=dry_run, session=session)
        
    if len(replicas) == 0:
        # No cache or cached list is all clear - do a one last scan
        replicas = scan_for_replicas(rse, rse_id, cache_path=cache_path, dry_run=dry_run, session=session)
        
    if len(replicas) == 0:
        # The RSE is now really completely cleared
        logger.info('Completed deleting the RSE contents')
        mark_rse_as_done(rse, rse_id, dry_run=dry_run, session=session)
        return True

    else:
        num_replicas = len(replicas)
        logger.warning('%(num_replicas)s replicas remain on the RSE even though there are no more rules creating locks on them.' % locals())
        return False


@transactional_session
def decommission_rse(rse, profile, limit_check, force=False, dry_run=False, log_level=logging.INFO, session=None):
    """
    Set RSE attributes for decommissioning (read=True, write=False, delete=True).

    :param rse:         Name of the RSE.
    :param profile:     Decommissioning profile identifier.
    :param limit_check: A LimitChecker instance.
    :param force:       If True, ignore the RSE attribute 'decommission'.
    :param session:     The database session.
    """

    # Define an RSE-specific logger for convenience
    logger = logging.getLogger('decommissioning.%(rse)s' % locals())
    logger.propagate = False
    handler = logging.StreamHandler(stream=sys.stdout)
    handler.setLevel(log_level)
    handler.setFormatter(logging.Formatter('%(asctime)s\t%(process)d\t%(levelname)s\tRSE={}: %(message)s'.format(rse)))
    logger.addHandler(handler)
    
    # Retrieve the RSE ID
    try:
        rse_id = get_rse_id(rse, session=session, include_deleted=False)
    except exception.RSENotFound:
        logger.warning('RSE %(rse)s not found' % locals())
        return False
    
    # Check the current decommissioning profile. This could be different from `profile` if the RSE was specified from command line
    profile_name = PROFILES[profile]

    logger.debug('Checking RSE attribute "decommission"')    
    try:
        current_state = get_rse_attribute('decommission', rse_id=rse_id, session=session)[0]
        
        if current_state != profile_name:
            if force:
                logger.warning('Current decommissioning state (profile) is "%(current_state)s"; switching to "%(profile_name)s" because force=True' % locals())
            else:
                logger.error('Current decommissioning state (profile) is "%(current_state)s"; aborting decommissioning with profile "%(profile_name)s"' % locals())
                return False
            
    except IndexError:
        logger.info('Setting RSE attribute decommission=%(profile_name)s' % locals())
        if not dry_run:
            add_rse_attribute(rse_id, 'decommission', profile_name, session=session)

    # Now run the decommissioning function according to the profile
    if profile == JUST_DELETE:
        return decommission_rse_just_delete(rse, rse_id, limit_check, dry_run=dry_run, session=session)

    elif profile == MOVE:
        return False
    

@transactional_session
def decommission_rses(rses=[],
                      profile=JUST_DELETE,
                      max_replicas=-1,
                      max_rules=-1,
                      max_datasets=-1,
                      force=False,
                      dry_run=True,
                      log_level=logging.INFO,
                      session=None):

    if not dry_run and max_replicas < 0 and max_rules < 0 and max_datasets < 0:
        raise RuntimeError('Throttling required for a non-dry run')
    
    profiles = {}
    
    # Loop over the RSEs with decommission attribute set
    query = session.query(models.RSE, models.RSEAttrAssociation).\
        join(models.RSEAttrAssociation).\
        filter(models.RSEAttrAssociation.key == 'decommission')
    
    for rse, attr in query.all():
        if attr.key == 'decommission':
            # RSE has the decommission flag set - flag value is either the profile string or 'complete'
            if attr.value == 'complete':
                continue
            elif attr.value == 'just_delete':
                profiles[rse.rse] = JUST_DELETE
            elif attr.value == 'move':
                profiles[rse.rse] = MOVE
            
    # Override by the function argument
    for rse in rses:
        profiles[rse] = profile
    
    profiles_text = dict((rse, PROFILES[profile]) for rse, profile in profiles.items())
    logging.info('RSEs to decommission: %(profiles_text)s' % locals())
     
    # Act on the RSEs
    for rse, profile in profiles.items():
        limit_check = LimitChecker(max_rules, max_datasets, max_replicas)
        decommission_rse(rse, profile, limit_check, force=force, dry_run=dry_run, log_level=log_level, session=session)

        
if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    
    decommission_rses(rses=args.rses,
                      max_replicas=args.max_replicas,
                      max_rules=args.max_rules,
                      max_datasets=args.max_datasets,
                      force=args.force,
                      dry_run=(not args.run),
                      log_level=getattr(logging, args.log_level.upper()))

