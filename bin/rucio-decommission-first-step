#!/usr/bin/env python

from __future__ import division
from __future__ import print_function

import sys
import re
from datetime import datetime
import logging
import argparse

from sqlalchemy import or_, and_
from sqlalchemy.sql import func
from sqlalchemy.orm.exc import NoResultFound

from rucio.core.rse import get_rse_id, get_rse_attribute, update_rse, add_rse_attribute, del_rse_attribute, get_rses_with_attribute
from rucio.core.rule import list_rules, update_rule
from rucio.core.rse_expression_parser import parse_expression
from rucio.common import exception
from rucio.common.config import config_get
from rucio.db.sqla import models
from rucio.db.sqla.session import read_session, transactional_session
from rucio.db.sqla.constants import DIDType

logging.basicConfig(stream=sys.stdout,
                    level=getattr(logging,
                                  config_get('common', 'loglevel',
                                             raise_exception=False,
                                             default='DEBUG').upper()),
                    format='%(asctime)s\t%(process)d\t%(levelname)s\t%(message)s')

def get_parser():
    """
    Returns the argparse parser.
    """
    parser = argparse.ArgumentParser(description='First step for writing a decommissioning agent.')
    parser.add_argument('--rse', dest='rses', metavar='RSE', action='store', nargs='+', default=[], help='RSE to decommission')
    parser.add_argument('--max-replicas', dest='max_replicas', action='store', type=int, default=-1, help='Maximum number of replicas to delete per cycle.')
    parser.add_argument('--max-rules', dest='max_rules', action='store', type=int, default=-1, help='Maximum number of rules to delete per cycle.')
    parser.add_argument('--max-datasets', dest='max_datasets', action='store', type=int, default=-1, help='Maximum number of datasets to delete per cycle.')
    #parser.add_argument('--dry-run', action='store_true', help='Dry run')
    parser.add_argument('--run', action='store_true', help='Actually run')
    return parser


class DecommissioningConfig(object):
    # Profile options
    _profiles = JUST_DELETE, MOVE = range(2)
    _profile_names = ['just_delete', 'move']

    def __init__(self, profile, **kwargs):
        if profile in self._profiles:
            self.profile = profile
        else:
            try:
                # Profile given by name?
                self.profile = self._profile_names.index(profile.lower())
            except:
                raise exception.ConfigurationError('Invalid profile specified for DecommissioningConfig')
        
        if self.profile == self.JUST_DELETE:
            # Set max_replicas, max_rules, and max_datasets possibly from kwargs (default -1)
            for var in ['replicas', 'rules', 'datasets']:
                try:
                    setattr(self, 'max_%s' % var, kwargs['max_%s' % var])
                except KeyError:
                    setattr(self, 'max_%s' % var, -1)
        else:
            pass


class DeletionLimitExceeded(Exception):
    def __init__(self, what, limit, count):
        self.what = what
        self.limit = limit
        self.count = count
        
    def __str__(self):
        return 'number of deleted %s %s reached the limit %s' % (self.what, self.count, self.limit)


class LimitChecker(object):
    def __init__(self, config):
        self.config = config

        self.deleted_replicas = 0
        self.deleted_rules = 0
        self.deleted_datasets = 0
        
        self._local_deleted_replicas = 0
        self._local_deleted_rules = 0
        self._local_deleted_datasets = 0
        
    def increment(self, replicas=0, rules=0, datasets=0):
        self.deleted_replicas += (replicas if replicas else 0)
        self.deleted_rules += (rules if rules else 0)
        self.deleted_datasets += (datasets if datasets else 0)
        
        self._local_deleted_replicas += (replicas if replicas else 0)
        self._local_deleted_rules += (rules if rules else 0)
        self._local_deleted_datasets += (datasets if datasets else 0)
        
        if self.config.max_replicas >= 0 and self.deleted_replicas >= self.config.max_replicas:
            raise DeletionLimitExceeded('replicas', self.config.max_replicas, self.deleted_replicas)
        
        if self.config.max_rules >= 0 and self.deleted_rules >= self.config.max_rules:
            raise DeletionLimitExceeded('rules', self.config.max_rules, self.deleted_rules)

        if self.config.max_datasets >= 0 and self.deleted_datasets >= self.config.max_datasets:
            raise DeletionLimitExceeded('datasets', self.config.max_datasets, self.deleted_datasets)

    def any(self):
        return self.deleted_replicas + self.deleted_rules + self.deleted_datasets != 0
    
    def start_local_count(self):
        """Local count within some code block"""
        self._local_deleted_replicas = 0
        self._local_deleted_rules = 0
        self._local_deleted_datasets = 0
        
    def local_any(self, reset=True):
        result = (self._local_deleted_replicas + self._local_deleted_rules + self._local_deleted_datasets != 0)
        if reset:
            self.start_local_count()
        return result
    

@transactional_session
def set_rse_availability(rse_id, log_header='', dry_run=False, session=None):
    """
    Get the RSE ready for decommissioning (availability and greedyDeletion)
    """
    
    # Check the RSE attributes
    update_dict = {}
    for attr, value in [('availability_read', True), ('availability_write', False), ('availability_delete', True), ('greedyDeletion', True)]:
        current = get_rse_attribute(attr, rse_id=rse_id, session=session)
        if current == value:
            continue
            
        logging.debug('%(log_header)supdating attribute %(attr)s (%(current)s => %(value)s)' % locals())
        update_dict[attr] = value

    # Update the attributes
    if update_dict and not dry_run:
        update_rse(rse_id, update_dict, session=session)

        
@transactional_session
def mark_rse_as_done(rse_id, log_header='', dry_run=False, session=None):
    logging.debug('%(log_header)supdating attribute decommission to "complete"' % locals())
    if not dry_run:
        # update_rse can only update attributes in the RSE table
        # custom RSE attributes are stored in RSEAttrAssociation
        # add_rse_attribute uses session.merge() which will update the attribute value if the key already exists
        add_rse_attribute(rse_id, 'decommission', 'complete', session=session)


@read_session
def __container_to_datasets(scope, name, max_depth=0, _depth=0, session=None):
    """Flatten a container to a list of datasets.

    ``scope`` should be a ``str`` with the scope of the container.

    ``name`` should be a ``str`` with the name of the container.

    ``max_depth`` should be a positive ``int`` to limit the recursion depth (default: no limit).

    ``session`` should be an ``sqlalchemy.orm.session.Session``-like
    object.  If not provided, one will be created automatically.

    Returns a ``list`` of ``tuples``s (scope, name).
    """
    
    if max_depth > 0 and _depth > max_depth:
        raise RuntimeError('exceeded maximum depth')
        
    query = session.query(models.DataIdentifierAssociation.child_scope,
                          models.DataIdentifierAssociation.child_name,
                          models.DataIdentifierAssociation.child_type).\
        with_hint(models.DataIdentifierAssociation, "INDEX(CONTENTS CONTENTS_PK)", 'oracle').\
        filter(models.DataIdentifierAssociation.scope == scope,
               models.DataIdentifierAssociation.name == name,
               or_(models.DataIdentifierAssociation.child_type == DIDType.CONTAINER,
                   models.DataIdentifierAssociation.child_type == DIDType.DATASET))
    
    datasets = set()

    for child_scope, child_name, child_type in query.all():
        if child_type == DIDType.CONTAINER:
            datasets.update(__container_to_datasets(child_scope, child_name, max_depth, _depth + 1, session))
        elif child_type == DIDType.DATASET:
            datasets.add((child_scope, child_name))
            
    return datasets


@transactional_session
def delete_single_rse_rules(rse, rse_id, limit_check, log_header='', dry_run=False, session=None):
    """
    Delete rules with RSE expressions that consist only of the name of the given RSE.
    
    :param rse:          Name of the RSE.
    :param rse_id:       RSE ID.
    :param limit_check:  LimitChecker instance
    :param session:      The database session.
    """
    
    limit_check.start_local_count()

    for rule in list_rules(filters={'rse_expression': rse}, session=session):
        if rule['expires_at'] is not None and rule['expires_at'] <= datetime.utcnow():
            # This rule is already bound for deletion
            continue
        
        rule_id = rule['id']
        scope = rule['scope']
        name = rule['name']
        did_type = rule['did_type']
        state = rule['state']
        
        type_str = str(did_type).lower()

        logging.debug('%(log_header)sDeleting rule %(rule_id)s for %(type_str)s %(scope)s:%(name)s' % locals())
        if not dry_run:
            options = {'lifetime': 0, 'state': 'suspended', 'cancel_requests': True}
            update_rule(rule_id, options, session=session)
        
        # Increment the counters for throttling
        if did_type == DIDType.FILE:
            limit_check.increment(rules=1, replicas=1)

        elif did_type in [DIDType.DATASET, DIDType.CONTAINER]:
            # This DID is a dataset or a container
            # We count all the constituents regardless of the replication state to throttle on the maximum load
            if did_type == DIDType.DATASET:
                dataset_condition = [and_(models.DataIdentifier.scope == scope, models.DataIdentifier.name == name)]
            elif did_type == DIDType.CONTAINER:
                dataset_condition = []
                for s, n in __container_to_datasets(scope, name, session=session):
                    dataset_condition.append(
                        and_(models.DataIdentifier.scope == s, models.DataIdentifier.name == n)
                    )

            num_datasets = len(dataset_condition)
            
            num_files = 0
            if num_datasets != 0:
                num_files, = session.query(func.sum(models.DataIdentifier.length)).\
                    filter(or_(*dataset_condition)).one()
                
            logging.debug('%(log_header)s%(num_datasets)s datasets and %(num_files)s files in this DID' % locals())

            # Finally increment the counters
            limit_check.increment(rules=1, datasets=num_datasets, replicas=num_files)
            
        else:
            raise NotImplementedError('delete_single_rse_rules can handle FILE, DATASET, and CONTAINER types at the moment.')
            
    if not limit_check.local_any(reset=True):
        logging.info('%(log_header)sFound no trivial rule to delete' % locals())

            
@transactional_session
def process_multi_rse_rules(rse, rse_id, limit_check, log_header='', dry_run=False, session=None):
    """
    Find rules with RSE expressions involving the given RSE among others.
    """
    
    limit_check.start_local_count()
    
    # Do we really want to loop over all rules and parse every single RSE expression? Is there not a better way?
    for rule in list_rules(session=session):
        rse_expression = rule['rse_expression']
        if rse_expression == rse:
            continue
            
        try:
            rse_info_list = parse_expression(rse_expression, session=session)
        except exception.InvalidRSEExpression:
            continue
            
        for rse_info in rse_info_list:
            if rse_info['rse'] == rse:
                scope = rule['scope']
                name = rule['name']
                # The rse_expression for this rule contains the current RSE among others
                logging.warning('%(log_header)sNontrivial rule for dataset %(scope)s:%(name)s. Deferring action' % locals())
                logging.debug('%(log_header)sRSE expression is "%(rse_expression)s"' % locals())
                limit_check.increment(rules=1) # Otherwise we'll mark this RSE as decommission=complete
                break
                
    if not limit_check.local_any(reset=True):
        logging.info('%(log_header)sFound no nontrivial rule to delete' % locals())


@transactional_session    
def decommission_rse_just_delete(rse, config, dry_run=False, session=None):
    try:
        rse_id = get_rse_id(rse, session=session, include_deleted=False)
    except exception.RSENotFound:
        logging.warning('RSE %(rse)s not found' % locals())
        return False
    
    limit_check = LimitChecker(config)
    
    log_header = 'RSE: %(rse)s -- ' % locals()

    try:
        # 1. Set RSE availability and greedyDeletion attributes
        logging.info('%(log_header)sSetting RSE availability flags' % locals())
        set_rse_availability(rse_id, log_header=log_header, dry_run=dry_run, session=session)
        
        # 2. Set RSE 'decommission' attribute to 'just_delete'
        logging.info('%(log_header)sSetting RSE attribute decommission=just_delete' % locals())
        if not dry_run:
            add_rse_attribute(rse_id, 'decommission', 'just_delete', session=session)

        # 3. Delete rules where the RSE expression is the RSE itself
        logging.info('%(log_header)sDeleting trivial rules on the RSE' % locals())
        delete_single_rse_rules(rse, rse_id, limit_check, log_header=log_header, dry_run=dry_run, session=session)

        # 4. Deal with rules with more complicated RSE expressions
        logging.info('%(log_header)sProcessing nontrivial rules' % locals())
        process_multi_rse_rules(rse, rse_id, limit_check, log_header=log_header, dry_run=dry_run, session=session)

        if not limit_check.any():
            # 4. There were no rules to be deleted in this cycle - move on to final cleanup
            # delete_locks_with_no_rule()
            # delete_replicas_with_no_locks()
            pass
    
    except DeletionLimitExceeded as exc:
        reason = str(exc)
        logging.info('%(log_header)sStopping decommissioning cycle because %(reason)' % locals())
        return False

    if not limit_check.any():
        # Nothing was deleted in this round - mark the RSE as decommissioning complete
        mark_rse_as_done(rse_id, log_header=log_header, dry_run=dry_run, session=session)
    
    return True


@transactional_session
def decommission_rse(rse, config, dry_run=False, session=None):
    """
    Set RSE attributes for decommissioning (read=True, write=False, delete=True).

    :param rse:      Name of the RSE.
    :param session:  The database session.
    """
    if config.profile == DecommissioningConfig.JUST_DELETE:
        return decommission_rse_just_delete(rse, config, dry_run=dry_run, session=session)

    elif config.profile == DecommissioningConfig.MOVE:
        return False


@read_session
def decommission_rses(rses=[],
                      profile=DecommissioningConfig.JUST_DELETE,
                      max_replicas=-1,
                      max_rules=-1,
                      max_datasets=-1,
                      dry_run=True,
                      session=None):

    configs = {}
    
    # Loop over the RSEs with decommission attribute set
    query = session.query(models.RSE, models.RSEAttrAssociation).\
        join(models.RSEAttrAssociation).\
        filter(models.RSEAttrAssociation.key == 'decommission')
    
    delete_limits = {}

    for rse, attr in query.all():
        if attr.key == 'decommission':
            # RSE has the decommission flag set - flag value is either the profile string or 'complete'
            if attr.value == 'complete':
                continue
            
            # If a profile string is given, create a configuration
            configs[rse.rse] = DecommissioningConfig(attr.value)

        elif attr.key.startswith('decommission_'):
            # Attributes 'decommission_*_per_run' sets the deletion limits
            matches = re.match('decommission_(replicas|rules|datasets)_per_run', attr.key)
            if matches is None:
                continue
                     
            delete_limits[(rse.rse, matches.group(1))] = int(attr.value)
        
    # Attach the deletion limits to the configurations
    for (rse, var), limit in delete_limits.items():
        try:
            setattr(configs[rse], 'max_%s' % var, limit)
        except KeyError:
            # Deletion limit was set but the RSE is not marked for decommissioning
            logging.warning('RSE %(rse)s has attribute decommission_%(var)s_per_run set, but is not marked for decommissioning' % locals())
            pass

    # Override by the function argument
    for rse in rses:
        config_args = {
            'max_replicas': max_replicas,
            'max_rules': max_rules,
            'max_datasets': max_datasets
        }
            
        configs[rse] = DecommissioningConfig(profile, **config_args)
        
    logging.info('RSEs to decommission: %(configs)s' % locals())
     
    # Act on the RSEs
    for rse, config in configs.items():
        decommission_rse(rse, config, dry_run=dry_run, session=session)

        
if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    
    decommission_rses(rses=args.rses,
                      max_replicas=args.max_replicas,
                      max_rules=args.max_rules,
                      max_datasets=args.max_datasets,
                      dry_run=(not args.run))